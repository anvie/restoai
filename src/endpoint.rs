use actix_web::{
    get, post,
    web::{self, Json},
    HttpRequest, HttpResponse, Responder,
};
use derive_more::{Deref, DerefMut, From};
use either::Either;
use futures::{Stream, StreamExt, TryStream};
use openai_dive::v1::resources::{
    chat::{ChatCompletionParameters, ChatMessage, DeltaToolCall, Role},
    model::ListModelResponse,
    shared::FinishReason,
};
use serde_derive::{self, Deserialize, Serialize};

use std::borrow::Cow;

use crate::{
    apitype,
    appctx::AppContext,
    llm::{LlmBackend, OpenAiBackend},
    streamer::{StreamChannel, Streamer},
};

type OAIAppContext = AppContext<OpenAiBackend>;

// #[derive(Debug, Serialize, Deserialize)]
// #[serde(tag = "role")]
// pub enum ChatMessage<'a> {
//     #[serde(rename = "system")]
//     System {
//         content: Option<Cow<'a, str>>,
//         name: Option<Cow<'a, str>>,
//     },
// }
//
// #[derive(Debug, Serialize, Deserialize, Default, Deref, DerefMut, From)]
// pub struct ChatMessages<'a>(
//     #[deref]
//     #[deref_mut]
//     Vec<ChatMessage<'a>>,
// );
//
// #[derive(Debug, Serialize, Deserialize)]
// pub struct CreateChatCompletionRequest<'a> {
//     #[serde(default)]
//     pub messages: ChatMessages<'a>,
//     pub max_tokens: Option<u32>,
//     pub temperature: Option<f32>,
//     pub top_p: Option<f32>,
//     pub presence_penalty: Option<f32>,
//     pub frequency_penalty: Option<f32>,
//     pub stream: Option<bool>,
//     pub one_shot: Option<bool>,
//     pub n: Option<u32>,
//     pub model: Cow<'a, str>,
//     pub seed: Option<u32>,
//
//     #[serde(default, with = "either::serde_untagged_optional")]
//     pub stop: Option<Either<Cow<'a, str>, Vec<Cow<'a, str>>>>,
// }

// #[derive(Debug, Serialize, Deserialize)]
// pub struct ModelList<'a> {
//     pub object: Cow<'a, str>,
//     pub data: Vec<Model>,
// }

pub type ModelList = ListModelResponse;

#[derive(Debug, Serialize, Deserialize)]
pub struct ChatCompletionChoice<'a> {
    /// The plaintext of the generated message.
    pub message: ChatMessage,

    /// If present, the reason that generation terminated at this choice.
    ///
    /// This can be:
    ///
    /// - `length`, indicating that the length cutoff was reached, or
    /// - `stop`, indicating that a stop word was reached.
    pub finish_reason: Option<Cow<'a, str>>,

    /// The index of this choice.
    pub index: u32,
}

impl From<openai_dive::v1::resources::chat::ChatCompletionChoice> for ChatCompletionChoice<'_> {
    fn from(choice: openai_dive::v1::resources::chat::ChatCompletionChoice) -> Self {
        Self {
            message: choice.message,
            finish_reason: serde_json::to_string(&choice.finish_reason).ok().map(Cow::Owned),
            index: choice.index,
        }
    }
}

#[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]
pub struct DeltaChatMessage {
    /// The role of the author of this message.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub role: Option<Role>,
    /// The contents of the chunk message.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub content: Option<String>,
    /// The tool calls generated by the model.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_calls: Option<Vec<DeltaToolCall>>,
}

#[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]
pub struct ChatCompletionChunkChoice {
    /// The index of the choice in the list of choices.
    pub index: Option<u32>,
    /// A chat completion delta generated by streamed model responses.
    pub delta: DeltaChatMessage,
    /// The reason the model stopped generating tokens.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub finish_reason: Option<FinishReason>,
}

impl From<openai_dive::v1::resources::chat::ChatCompletionChunkChoice> for ChatCompletionChunkChoice {
    fn from(choice: openai_dive::v1::resources::chat::ChatCompletionChunkChoice) -> Self {
        Self {
            index: choice.index,
            delta: DeltaChatMessage {
                role: choice.delta.role,
                content: choice.delta.content.map(|c| c.to_string()),
                tool_calls: choice.delta.tool_calls,
            },
            finish_reason: choice.finish_reason,
        }
    }
}

// type ChatCompletionChoices<'a> = Vec<ChatCompletionChoice<'a>>;
//
// impl From<Vec<openai_dive::v1::resources::chat::ChatCompletionChoice>> for ChatCompletionChoices<'_> {
//     fn from(choices: Vec<openai_dive::v1::resources::chat::ChatCompletionChoice>) -> Self {
//         choices.into_iter().map(ChatCompletionChoice::from).collect()
//     }
// }

#[derive(Debug, Serialize, Deserialize)]
pub struct ChatCompletionUsage {
    /// The number of generated tokens.
    pub completion_tokens: u32,

    /// The number of tokens in the prompt.
    pub prompt_tokens: u32,

    /// `completion_tokens` + `prompt_tokens`; the total number of tokens in the dialogue
    /// so far.
    pub total_tokens: u32,
}

#[derive(Debug, Serialize, Deserialize, Default)]
pub struct ChatCompletion<'a> {
    pub id: Cow<'a, str>,
    pub choices: Vec<ChatCompletionChoice<'a>>,

    pub created: u32,
    //pub model: Cow<'a, str>,
    //pub system_fingerprint: Cow<'a, str>,
    pub object: Cow<'a, str>,
    pub usage: Option<ChatCompletionUsage>,
}
//
// enum ChatCompletionResponse<'a, S>
// where
//     S: TryStream<Ok = Event> + Send + 'static,
// {
//     Full(Json<ChatCompletion<'a>>),
//     Stream(Sse<S>),
// }
//

#[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]
pub struct ChatCompletionChunkResponse<'a> {
    /// A unique identifier for the chat completion. Each chunk has the same ID.
    pub id: Cow<'a, str>,
    /// A list of chat completion choices. Can be more than one if n is greater than 1.
    pub choices: Vec<ChatCompletionChunkChoice>,
    /// The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.
    pub created: u32,
    /// The model to generate the completion.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub model: Option<String>,
    /// This fingerprint represents the backend configuration that the model runs with.
    /// Can be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub system_fingerprint: Option<String>,
    /// The object type, which is always chat.completion.chunk.
    pub object: Cow<'a, str>,
}

#[derive(Debug, Deserialize)]
struct TestData {
    pub name: String,
}

#[post("/chat/completions")]
pub async fn chat_completions(data: web::Json<ChatCompletionParameters>, ctx: web::Data<OAIAppContext>) -> impl Responder {
    let stream_channel: StreamChannel = ctx.streamer.new_client().await;

    // let writer = stream_channel
    //     .get_stream_inner(stream_channel.id)
    //     .expect("Failed to get stream writer");
    //
    let writer = stream_channel.get_stream_writer();

    let messages = data
        .messages
        .iter()
        .map(|m| ChatMessage {
            role: m.role.clone(),
            content: m.content.clone(),
            name: m.name.clone(),
            ..Default::default()
        })
        .collect();

    let llm_backend = ctx.llm_backend.clone();

    tokio::spawn(async move {
        llm_backend.submit_prompt(messages, writer).await;
    });

    stream_channel.stream
}

#[post("/chat/broadcast")]
pub async fn broadcast(data: web::Json<TestData>, ctx: web::Data<OAIAppContext>) -> impl Responder {
    ctx.streamer.broadcast(&format!("hello {}!", &data.name)).await;
    HttpResponse::Ok().body("Sent.")
}

lazy_static! {
    static ref SUPPORTED_MODELS: Vec<&'static str> = vec!["programmer", "sysadmin",];
}

#[get("/models")]
pub async fn models(ctx: web::Data<OAIAppContext>) -> impl Responder {
    let models = ctx.llm_backend.models().await;

    let models = apitype::ListModelResponse {
        object: models.object.into(),
        // data: models
        //     .data
        //     .into_iter()
        //     .map(|d| openai_dive::v1::resources::model::Model {
        //         id: d.id,
        //         object: d.object,
        //         created: d.created,
        //         owned_by: d.owned_by,
        //     })
        //     .collect(),
        data: SUPPORTED_MODELS
            .iter()
            .map(|m| apitype::Model {
                id: (*m).into(),
                object: "model".into(),
                created: 0,
                owned_by: None,
            })
            .collect(),
    };

    HttpResponse::Ok().json(models)
}

// pub async fn chat_completions(Json(req) = Json<CreateChatCompletionRequest<'_>>) -> Result<impl IntoResponse, ChatCompletionError> {
//
//
//
// }
//
